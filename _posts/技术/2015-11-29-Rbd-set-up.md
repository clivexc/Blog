---
layout: post
title: Ceph rbd客户使用端
category: 技术
tags: Ceph
keywords: 
description: 
---
Rhel7.1已经支持内核RBD，在已有Ceph集群的前提下，使用RBD的过程分享如下：

##1.创建pool和image
ceph osd pool create {pool-name} {pg-num}

rbd create {img-name} --size {int} -p {pool-name}

##2.映射虚拟磁盘
rbd map {pool-name}/{img-name}

    [root@node1 ~]# rbd showmapped
    id pool  image snap device    
    0  clive test  -    /dev/rbd0

##3.文件测试
准备一个test文件，写点内容，然后使用dd命令写入集群：

	 dd if=test of=/dev/rbd0

生成对象：
 
	 [root@node0 ~]# rados ls -p clive
	 rbd_directory
	 img.rbd
	 rb.0.2a864.6b8b4567.000000000000

对象存放路径：

	 [root@node0 ~]# ceph osd map clive rb.0.2a864.6b8b4567.000000000000
	 osdmap e153 pool 'clive' (2) object 'rb.0.2a864.6b8b4567.000000000000' -> pg 2.2a336b6d (2.2d) -> up ([1], p1) acting ([1], p1)

	 [root@node0 ~]# cd /var/lib/ceph/osd/ceph-1/current/2.2d_head/
	 [root@node0 2.2d_head]# ll
	 total 1028
	 -rw-r--r--. 1 root root    0 Aug 27 09:28 __head_0000002D__2
	 -rw-r--r--. 1 root root 4096 Aug 27 09:42 rb.0.2a864.6b8b4567.000000000000__head_2A336B6D__2


文件导出： 

	[root@node0 2.2d_head]# dd if=/dev/rbd0 of=/root/out bs=100 count=1
	1+0 records in
	1+0 records out
	100 bytes (100 B) copied, 0.00533822 s, 18.7 kB/s

看看/root/out 与 test的内容是否一样
